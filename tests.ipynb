{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Activation, BatchNormalization, Flatten\n",
    "from keras.layers import UpSampling2D, Conv2D, MaxPool2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "LRELU = 0.2\n",
    "\n",
    "def custom_conv(x, filters, kernel=3, bn=True, activation='relu', ud_sample='None'):\n",
    "    if ud_sample == 'up':\n",
    "        x = UpSampling2D()(x)\n",
    "    \n",
    "    initialization = 'he_uniform' if activation == 'relu' else 'glorot_uniform'\n",
    "    out = Conv2D(filters, kernel, padding='same', kernel_initializer=initialization)(x)\n",
    "    if bn:\n",
    "        out = BatchNormalization()(out)\n",
    "    out = Activation(activation)(out)\n",
    "    \n",
    "    if ud_sample == 'down':\n",
    "        out = MaxPool2D()(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def custom_dense(x, units, bn=True, activation='lrelu'):\n",
    "    initialization = 'he_uniform' if activation.find('relu') == -1 else 'glorot_uniform'\n",
    "    activation_fn = LeakyReLU(LRELU) if activation == 'lrelu' else Activation(activation)\n",
    "    out = Dense(units, kernel_initializer=initialization)(x)\n",
    "    if bn:\n",
    "        out = BatchNormalization()(out)\n",
    "    out = activation_fn(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def generator_model(latent_dims):\n",
    "    x = Input((latent_dims,))\n",
    "    y = custom_dense(x, 1024)\n",
    "    y = custom_dense(y, 128*7*7)\n",
    "    y = Reshape((7,7,128))(y)\n",
    "    y = custom_conv(y, 64, 5, ud_sample='up')\n",
    "    y = custom_conv(y, 1, 5, ud_sample='up', activation='tanh', bn=False)\n",
    "    model = Model(x, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def discriminator_model(input_shape=(28,28,1)):\n",
    "    x = Input(input_shape)\n",
    "    y = custom_conv(x, 64, 5, ud_sample='down')\n",
    "    y = custom_conv(y, 128, 5, ud_sample='down')\n",
    "    y = Flatten()(y)\n",
    "    y = custom_dense(y, 1024)\n",
    "    y = custom_dense(y, 1, bn=False, activation='sigmoid')\n",
    "    model = Model(x, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GanModel(object):\n",
    "    def __init__(self, g_weights='generator.h5', d_weights='discriminator.h5', data='data.npy'\n",
    "                 lr=5e-4, latent_dims=100):\n",
    "        self.lr = lr\n",
    "        self.latent_dims = latent_dims\n",
    "        self.d_weights = d_weights\n",
    "        self.g_weights = g_weights\n",
    "        self.d = discriminator_model()\n",
    "        self.g = generator_model(latent_dims)\n",
    "        self.dg = generator_containing_discriminator(self.g, self.d)\n",
    "        self.g.compile(loss='binary_crossentropy', optimizer=Adam(self.lr))\n",
    "        self.dg.compile(loss='binary_crossentropy', optimizer=Adam(self.lr))\n",
    "        self.d.trainable = True\n",
    "        self.d.compile(loss='binary_crossentropy', optimizer=Adam(self.lr))\n",
    "        self.load_weights()\n",
    "        self.d_loss = []\n",
    "        self.g_loss = []\n",
    "        \n",
    "    def load_weights(self):\n",
    "        if os.path.exists(self.g_weights) and os.path.exists(self.d_weights):\n",
    "            self.d.load_weights(self.d_weights)\n",
    "            self.g.load_weights(self.g_weights)\n",
    "        \n",
    "    def train(self, train_data, test_data=None, batch_size=128, epochs=20):\n",
    "        X_train, y_train = train_data\n",
    "        if test_data:\n",
    "            X_test, y_test = test_data\n",
    "        \n",
    "        n_batches = X_train.shape[0] // BATCH_SIZE\n",
    "        noise = np.zeros((batch_size, self.latent_dims))\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch : {epoch}')\n",
    "            for index in range(n_batches):\n",
    "                for i in range(batch_size):\n",
    "                    noise[i, :] = np.random.uniform(-1, 1, self.latent_dims)\n",
    "                    \n",
    "                image_batch = X_train[index*batch_size:(index+1)*batch_size]\n",
    "                generated_images = self.g.predict(noise, verbose=0)\n",
    "                X = np.concatenate((image_batch, generated_images))\n",
    "                y = [0.9] * batch_size + [0.0] * batch_size\n",
    "                d_loss = self.d.train_on_batch(X, y)\n",
    "                self.d_loss.append(d_loss)\n",
    "                print('batch %d d_loss : %.5f' % (index, d_loss))\n",
    "                for i in range(batch_size):\n",
    "                    noise[i, :] = np.random.uniform(-1, 1, self.latent_dims)\n",
    "                    \n",
    "                self.d.trainable = False\n",
    "                g_loss = self.dg.train_on_batch(noise, [1] * batch_size)\n",
    "                discriminator.trainable = True\n",
    "                self.g_loss.append(g_loss)\n",
    "                print('batch %d g_loss : %.5f' % (index, g_loss))\n",
    "                if index % 10 == 9:\n",
    "                    self.g.save_weights(self.g_weights)\n",
    "                    self.d.save_weights(self.d_weights)\n",
    "                    \n",
    "    def generate(self, batch_size):\n",
    "        noise = np.zeros((batch_size, self.latent_dims))\n",
    "        for i in range(BATCH_SIZE):\n",
    "            noise[i, :] = np.random.uniform(-1, 1, self.latent_dims)\n",
    "            \n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        \n",
    "        return generated_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
