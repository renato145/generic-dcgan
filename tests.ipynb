{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Activation, BatchNormalization, Flatten\n",
    "from keras.layers import UpSampling2D, Conv2D, MaxPool2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "LRELU = 0.2\n",
    "\n",
    "def custom_conv(x, filters, kernel=3, bn=True, activation='relu', ud_sample='None'):\n",
    "    if ud_sample == 'up':\n",
    "        x = UpSampling2D()(x)\n",
    "    \n",
    "    initialization = 'he_uniform' if activation == 'relu' else 'glorot_uniform'\n",
    "    out = Conv2D(filters, kernel, padding='same', kernel_initializer=initialization)(x)\n",
    "    if bn:\n",
    "        out = BatchNormalization()(out)\n",
    "    out = Activation(activation)(out)\n",
    "    \n",
    "    if ud_sample == 'down':\n",
    "        out = MaxPool2D()(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def custom_dense(x, units, bn=True, activation='lrelu'):\n",
    "    initialization = 'he_uniform' if activation.find('relu') == -1 else 'glorot_uniform'\n",
    "    activation_fn = LeakyReLU(LRELU) if activation == 'lrelu' else Activation(activation)\n",
    "    out = Dense(units, kernel_initializer=initialization)(x)\n",
    "    if bn:\n",
    "        out = BatchNormalization()(out)\n",
    "    out = activation_fn(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def generator_model(input_shape=(100,)):\n",
    "    x = Input(input_shape)\n",
    "    y = custom_dense(x, 1024)\n",
    "    y = custom_dense(y, 128*7*7)\n",
    "    y = Reshape((7,7,128))(y)\n",
    "    y = custom_conv(y, 64, 5, ud_sample='up')\n",
    "    y = custom_conv(y, 1, 5, ud_sample='up', activation='tanh', bn=False)\n",
    "    model = Model(x, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def discriminator_model(input_shape=(28,28,1)):\n",
    "    x = Input(input_shape)\n",
    "    y = custom_conv(x, 64, 5, ud_sample='down')\n",
    "    y = custom_conv(y, 128, 5, ud_sample='down')\n",
    "    y = Flatten()(y)\n",
    "    y = custom_dense(y, 1024)\n",
    "    y = custom_dense(y, 1, bn=False, activation='sigmoid')\n",
    "    model = Model(x, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE=128):\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
    "    discriminator = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    discriminator_on_generator = \\\n",
    "        generator_containing_discriminator(generator, discriminator)\n",
    "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    discriminator_on_generator.compile(\n",
    "        loss='binary_crossentropy', optimizer=g_optim)\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    noise = np.zeros((BATCH_SIZE, 100))\n",
    "    for epoch in range(100):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images = generator.predict(noise, verbose=0)\n",
    "            if index % 20 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = discriminator_on_generator.train_on_batch(\n",
    "                noise, [1] * BATCH_SIZE)\n",
    "            discriminator.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                generator.save_weights('generator', True)\n",
    "                discriminator.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generator_model()\n",
    "d = discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = generator_containing_discriminator(g, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.trainable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
