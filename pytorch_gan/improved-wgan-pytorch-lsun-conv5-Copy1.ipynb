{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = Variable(torch.randn([2,1,32,32,32]).cuda())\n",
    "# real loss\n",
    "netD.zero_grad()\n",
    "errD_real = step_D(real, mone)\n",
    "# fake loss\n",
    "noise = Variable(torch.randn(2, 128).cuda(), volatile=True)\n",
    "fake = Variable(netG(noise).data)\n",
    "errD_fake = step_D(fake, one)\n",
    "# Gradient penalty (improved wgan paper):\n",
    "gradient_penalty = calc_gradient_penalty(netD, real.data, fake.data)\n",
    "gradient_penalty.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 32, 32, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = G(128).cuda()\n",
    "noise = Variable(torch.randn(2, 128).cuda(), volatile=True)\n",
    "fake = Variable(netG(noise).data)\n",
    "fake.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = D(128).cuda()\n",
    "# netD(fake).size()\n",
    "errD_fake = step_D(fake, one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 32, 32, 32]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "real = Variable(torch.randn([2,1,32,32,32]).cuda())\n",
    "gradient_penalty = calc_gradient_penalty(netD, real.data, fake.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ConvNdBackward: expected Variable at argument 0 (got None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c4f1190b3b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ConvNdBackward: expected Variable at argument 0 (got None)"
     ]
    }
   ],
   "source": [
    "gradient_penalty.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(real_data.size()[0],1,1,1,1)\n",
    "    alpha = alpha.expand(real_data.size()).cuda()\n",
    "    interpolates = (alpha * real_data) + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "    print(interpolates.size(), disc_interpolates.size())\n",
    "    inputs = [interpolates] + list(netD.parameters())\n",
    "    \n",
    "    gradients = grad(outputs=disc_interpolates, inputs=inputs,\n",
    "                     grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients[0].norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    \n",
    "    for gradi in gradients[1:]:\n",
    "        gradient_penalty += 0 * gradi.view(-1)[0]\n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(real_data.size()[0],1,1,1,1)\n",
    "    alpha = alpha.expand(real_data.size()).cuda()\n",
    "    interpolates = (alpha * real_data) + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "    print(interpolates.size(), disc_interpolates.size())\n",
    "    gradients = grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                     grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(D, self).__init__()\n",
    "        self.l1 = nn.Conv3d(1, 2, 4, 2, 1, bias=False)\n",
    "        self.l2 = nn.Conv3d(2, 4, 4, 2, 1, bias=False)\n",
    "        self.l3 = nn.Conv3d(4, 8, 4, 2, 1, bias=False)\n",
    "        self.l4 = nn.Conv3d(8, 8, 4, 2, 1, bias=False)\n",
    "        self.l5 = nn.Linear(8*2*2*2, 128)\n",
    "        self.l6 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        out = out.view([-1,8*2*2*2])\n",
    "        out = self.l5(out)\n",
    "        out = self.l6(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(G, self).__init__()\n",
    "        self.features = features\n",
    "        self.l1 = nn.Linear(features, 4*4*4)\n",
    "        self.l2 = nn.ConvTranspose3d(1, 4, 4, 2, 1, bias=False)\n",
    "        self.l3 = nn.ConvTranspose3d(4, 8, 4, 2, 1, bias=False)\n",
    "        self.l4 = nn.ConvTranspose3d(8, 1, 4, 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = out.view([-1,1,4,4,4])\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(D, self).__init__()\n",
    "        self.l1 = nn.Conv2d(1, 2, 4, 2, 1, bias=False)\n",
    "        self.l2 = nn.Conv2d(2, 4, 4, 2, 1, bias=False)\n",
    "        self.l3 = nn.Conv2d(4, 8, 4, 2, 1, bias=False)\n",
    "        self.l4 = nn.Conv2d(8, 8, 4, 2, 1, bias=False)\n",
    "        self.l5 = nn.Linear(8*2*2, 128)\n",
    "        self.l6 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        out = out.view([-1,8*2*2])\n",
    "        out = self.l5(out)\n",
    "        out = self.l6(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(G, self).__init__()\n",
    "        self.features = features\n",
    "        self.l1 = nn.Linear(features, 4*4)\n",
    "        self.l2 = nn.ConvTranspose2d(1, 4, 4, 2, 1, bias=False)\n",
    "        self.l3 = nn.ConvTranspose2d(4, 8, 4, 2, 1, bias=False)\n",
    "        self.l4 = nn.ConvTranspose2d(8, 1, 4, 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = out.view([-1,1,4,4])\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Wasserstein GAN in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms, utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we, set up batch size, image size, and size of noise vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs,sz,nz = 256,64,128\n",
    "LAMBDA = 10\n",
    "PATH_G = 'iwgancv5_g'\n",
    "PATH_D = 'iwgancv5_d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = '/home/m20163692/data/lsun'\n",
    "data = datasets.LSUN(db_path=PATH, classes=['church_outdoor_train'],\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Scale(sz),\n",
    "        transforms.CenterCrop(sz),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even parallel processing is handling automatically by torch-vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(data, bs, True, num_workers=8, pin_memory=True)\n",
    "n = len(dataloader); n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader))[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our activation function will be `tanh`, so we need to do some processing to view the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(img, fs=(8,8)):\n",
    "    plt.figure(figsize = fs)\n",
    "    plt.imshow(np.transpose((img/2+0.5).clamp(0,1).numpy(), (1,2,0)), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch uses `module.apply()` for picking an initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netG = G(nz).cuda()\n",
    "netG.apply(weights_init);\n",
    "\n",
    "netD = D(nz).cuda()\n",
    "netD.apply(weights_init);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just some shortcuts to create tensors and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import FloatTensor as FT\n",
    "\n",
    "def Var(*params):\n",
    "    return Variable(FT(*params).cuda())\n",
    "\n",
    "def create_noise(b, volatile=False): \n",
    "    return Variable(torch.randn(b, nz).cuda(), volatile=volatile)\n",
    "# Variable(FT(b, nz).cuda().normal_(0, 1), volatile=volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input placeholder\n",
    "input = Var(bs, 3, sz, nz)\n",
    "# Fixed noise used just for visualizing images when done\n",
    "fixed_noise = create_noise(bs)\n",
    "# The numbers 1 and -1\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam parameters from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADAM\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 1e-4, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One forward step and one backward step for D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)\n",
    "real = Variable(next(data_iter)[0].cuda())\n",
    "real.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = netG(create_noise(real.size()[0]))\n",
    "fake.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alpha = torch.rand(bs,1,1,1).cuda()\n",
    "# alpha = alpha.expand_as(real)\n",
    "# alpha.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interpolates = (alpha * real.data) + ((1 - alpha) * fake.data)\n",
    "# interpolates.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interpolates = torch.autograd.Variable(interpolates, requires_grad=True)\n",
    "# D_interpolates = netD(interpolates)\n",
    "# D_interpolates.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradients = grad(D_interpolates, interpolates, create_graph=True)[0]\n",
    "# gradients.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "# gradient_penalty.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D_loss = D_fake - D_real + LAMBDA * gradient_penalty\n",
    "# D_loss.backward()\n",
    "# optimizerD.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_D(v, init_grad):\n",
    "    err = netD(v).mean()\n",
    "    err.backward(init_grad)\n",
    "\n",
    "    return err\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(real_data.size()[0],1,1,1)\n",
    "    alpha = alpha.expand(real_data.size()).cuda()\n",
    "    interpolates = (alpha * real_data) + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "    print(interpolates.size(), disc_interpolates.size())\n",
    "    gradients = grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                     grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "\n",
    "    return gradient_penalty\n",
    "\n",
    "def make_trainable(net, val): \n",
    "    for p in net.parameters(): p.requires_grad = val\n",
    "\n",
    "def train(niter, first=True):\n",
    "    netG.train()\n",
    "    gen_iterations = 0\n",
    "    save_flag = 1\n",
    "    for epoch in range(niter):\n",
    "        t0 = time()\n",
    "        data_iter = iter(dataloader)\n",
    "        i = 0\n",
    "        \n",
    "        while i < n:\n",
    "            make_trainable(netD, True)\n",
    "            #d_iters = (50 if first and (gen_iterations < 5) or gen_iterations % 500 == 0 \n",
    "            #           else 5)\n",
    "            d_iters = 5\n",
    "            j = 0\n",
    "            while j < d_iters and i < n:\n",
    "                j += 1; i += 1\n",
    "                print(f'{i:04}/{n:04} - G_iterations: {gen_iterations:04}', end='\\r')\n",
    "                real = Variable(next(data_iter)[0].cuda())\n",
    "                # real loss\n",
    "                netD.zero_grad()\n",
    "                errD_real = step_D(real, mone)\n",
    "                # fake loss\n",
    "                noise = create_noise(real.size()[0], volatile=True)\n",
    "                fake = Variable(netG(noise).data)\n",
    "                errD_fake = step_D(fake, one)\n",
    "                # Gradient penalty (improved wgan paper):\n",
    "                gradient_penalty = calc_gradient_penalty(netD, real.data, fake.data)\n",
    "                gradient_penalty.backward()\n",
    "                errD = errD_fake - errD_real + gradient_penalty\n",
    "                optimizerD.step()\n",
    "#                 print('D: %.3f | DF: %.3f | DR: %.3f | GP: %.3f' %\n",
    "#                       (errD.data[0], errD_fake.data[0], errD_real.data[0], gradient_penalty.data[0]))\n",
    "                \n",
    "            make_trainable(netD, False)\n",
    "            netG.zero_grad()\n",
    "            errG = -step_D(netG(create_noise(bs)), mone)\n",
    "            optimizerG.step()\n",
    "            gen_iterations += 1\n",
    "            if save_flag == -1:\n",
    "                p_g = PATH_G + '_temp.pkl'\n",
    "                p_d = PATH_D + '_temp.pkl'\n",
    "            else:\n",
    "                p_g = PATH_G + '.pkl'\n",
    "                p_d = PATH_D + '.pkl'\n",
    "            \n",
    "            torch.save(netG.state_dict(), p_g)\n",
    "            torch.save(netD.state_dict(), p_d)\n",
    "            save_flag *= -1\n",
    "            \n",
    "            with open('logs/log.txt', 'a') as f:\n",
    "                f.writelines('[%d/%d][%d/%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f (%.2fs)\\n' %\n",
    "                             (epoch, niter, gen_iterations, n, errD.data[0], errG.data[0],\n",
    "                              errD_real.data[0], errD_fake.data[0], time() - t0))\n",
    "            \n",
    "        print('[%d/%d][%d/%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f (%.2fs)' % (\n",
    "            epoch, niter, gen_iterations, n,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0], time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train(50000, True) # First run\n",
    "# train(10000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_g = PATH_G + '.pkl'\n",
    "p_d = PATH_D + '.pkl'\n",
    "torch.save(netG.state_dict(), p_g)\n",
    "torch.save(netD.state_dict(), p_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD\n",
    "netG.load_state_dict(torch.load(PATH_G + '.pkl'))\n",
    "netD.load_state_dict(torch.load(PATH_D + '.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model mode eval\n",
    "netG.eval()\n",
    "fixed_noise = create_noise(64)\n",
    "fake = netG(fixed_noise).data.cpu()\n",
    "# show(random.choice(fake))\n",
    "show(utils.make_grid(fake), (16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def animate(i):\n",
    "    ax.imshow(np.transpose((interpolation[i]/2+0.5).clamp(0,1).numpy(), (1,2,0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [4,52]\n",
    "noise = fixed_noise[idxs, :]\n",
    "img = netG(noise).data.cpu()\n",
    "show(utils.make_grid(img), (7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_imgs = 100\n",
    "diff = (noise[1].data - noise[0].data) / n_imgs\n",
    "noise_interpolation = torch.arange(0, n_imgs+1).cuda()\n",
    "noise_interpolation = noise_interpolation.repeat(nz, 1).transpose(0, 1)\n",
    "noise_interpolation = noise[0].data + (diff * noise_interpolation)\n",
    "\n",
    "interpolation = netG(Variable(noise_interpolation, volatile=True)).data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show(utils.make_grid(interpolation, nrow=int(np.sqrt(n_imgs))), (16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, animate, frames=n_imgs, interval=100)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(utils.make_grid(iter(dataloader).next()[0][:64]), (16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
